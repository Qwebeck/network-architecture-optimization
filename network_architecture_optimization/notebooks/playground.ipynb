{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:15:39.067210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:39.067256: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-06 11:15:43.241056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-06 11:15:43.241359: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241421: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241502: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241579: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241615: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-06 11:15:43.241659: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-06 11:15:43.316011: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:15:47,223\tWARNING function_runner.py:561 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:47.941170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:47.941196: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:49.099976: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:49.100118: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kbPC\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:49.100153: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kbPC\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:49.100332: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.54.0\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:49.100387: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.54.0\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:49.100418: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.54.0\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m 2022-04-06 11:15:49.101006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5672)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-06 11:15:49 (running for 00:00:01.96)<br>Memory usage on this node: 5.3/13.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 320.000: None | Iter 80.000: None | Iter 20.000: None<br>Resources requested: 2.0/16 CPUs, 0/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.98 GiB objects<br>Result logdir: /home/kdbogusz/ray_results/exp<br>Number of trials: 3/3 (2 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">       lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_25056_00000</td><td>RUNNING </td><td>192.168.1.9:5672</td><td style=\"text-align: right;\">0.0467722</td></tr>\n",
       "<tr><td>lambda_25056_00001</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0341257</td></tr>\n",
       "<tr><td>lambda_25056_00002</td><td>PENDING </td><td>                </td><td style=\"text-align: right;\">0.0133466</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 11:15:49,482\tERROR trial_runner.py:920 -- Trial lambda_25056_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/worker.py\", line 1763, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5672, ip=192.168.1.9, repr=<lambda>)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/trainable.py\", line 319, in train\n",
      "    result = self.step()\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 381, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 531, in _report_thread_runner_error\n",
      "    raise TuneError(\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5672, ip=192.168.1.9, repr=<lambda>)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "    lambda cfg: self.train_model(cfg),\n",
      "  File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "    model.fit(\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "AssertionError: in user code:\n",
      "\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\n",
      "    AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "    \n",
      "    Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "    \n",
      "    Call arguments received:\n",
      "      • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "      • training=True\n",
      "      • mask=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Model: \"model\"\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m _________________________________________________________________\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  Layer (type)                Output Shape              Param #   \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m =================================================================\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  input_1 (InputLayer)        [(None, 8, 8)]            0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  flatten (Flatten)           (None, 64)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  dense_1 (Dense)             (None, 64)                4160      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  dense_5 (Dense)             (None, 64)                4160      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  dense_10 (Dense)            (None, 16)                1040      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  dense_15 (Dense)            (None, 8)                 136       \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  gaussian_dropout (GaussianD  (None, 8)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  ropout)                                                         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  dense_16 (Dense)            (None, 10)                90        \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  softmax (Softmax)           (None, 10)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m =================================================================\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Total params: 9,586\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Trainable params: 9,586\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m _________________________________________________________________\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m ===\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m ===\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m [[[ 0.  0.  1. ...  9.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 13. ... 11.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 13. ...  9.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 11. ... 16.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  9. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  3. ...  2.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  8. ...  3.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  6. 15. ... 12.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 15.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 11. ... 15.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  8. ... 12.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  1. 10. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  7. 14. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  7. 11. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  4. 12.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 11. ... 10. 12.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 10. ... 14.  4.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  7. 12. ...  8.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  8. 16. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  2. 11. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  7. 16. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  4. 16. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  9. 12. ...  0.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  7. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  6. ... 16. 12.  5.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  5. ... 16. 16. 15.]]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m [8 1 3 ... 5 7 1]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m [[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  5.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  8. 12. ... 15.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 10.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  3.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  6. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  5. ... 10.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 15.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  9.  4.  1.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  3. ... 16. 16. 10.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  5. ...  9.  6.  2.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  0. ... 14.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 16.  2.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 14.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  6. 16. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 16.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 14.  1.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  0. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ...  9.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 15. ...  4.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  3. 12. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  0. ... 13.  7.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  0. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  8. ...  4.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  3. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  2. 16. ... 13.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 11. ... 16.  9.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  1. ... 10.  2.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  [[ 0.  0.  6. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0. 15. ... 15.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  4. 16. ...  9.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  4.  8. ...  9.  4.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  1. 13. ... 14.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   [ 0.  0.  5. ...  4.  0.  0.]]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m [4 2 1 1 0 6 7 9 5 6 7 0 3 6 7 5 4 3 6 3 7 7 4 5 0 9 3 1 2 8 0 4 0 0 3 7 1\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  1 3 5 1 2 9 4 1 0 8 5 3 1 7 6 5 3 0 2 1 2 3 3 8 2 8 3 4 7 4 9 0 6 2 1 0 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  9 8 2 0 6 4 7 5 5 5 3 5 3 3 9 6 3 5 9 1 1 8 5 9 4 2 6 9 8 4 1 2 6 2 0 7 1\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  3 9 1 4 6 0 4 5 2 7 5 2 5 9 4 9 9 2 2 0 1 8 7 9 9 9 3 3 1 0 8 6 0 2 4 1 9\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  2 0 2 1 5 7 2 1 0 9 7 9 4 0 2 3 2 8 1 9 9 3 7 6 5 1 5 2 7 4 6 8 8 4 5 0 5\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  3 3 4 0 1 8 2 3 4 0 4 4 6 6 6 4 3 1 9 3 9 0 1 1 4 2 6 4 2 7 7 9 5 8 6 6 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  6 8 2 9 1 5 0 5 1 8 7 8 6 6 5 1 0 5 9 0 6 4 3 4 9 3 9 5 2 7 2 6 7 1 1 1 8\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  6 8 8 1 6 1 7 6 8 4 5 2 8 2 3 7 6 2 8 6 8 6 7 5 3 7 1 0 1 7 4 0 9 2 5 8 5\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  8 2 7 7 8 6 2 1 6 7 1 2 5 9 2 3 9 7 2 8 7 2 4 5 1 6 0 0 3 6 8 7 4 9 6 9 4\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  6 2 2 5 2 6 2 4 5 3 9 9 4 0 0 4 8 2 7 8 9 7 1 3 2 2 3 2 7 7 4 1 7 2 4 9 7\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  5 3 9 9 4 9 4 9 2 9 8 9 5 9 7 7 5 0 8 5 7 2 4 7 8 7 6 6 8 9 0 8 5 5 8 8 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  3 6 0 7 4 1 5 4 9 5 9 6 1 2 7 5 6 3 1 0 4 3 0 0 8 0 7 0 9 7 3 2 9 6 9 0 9\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m  4 6 7 1 6 0]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Epoch 1/12\n",
      "Result for lambda_25056_00000:\n",
      "  date: 2022-04-06_11-15-49\n",
      "  experiment_id: 6cbf8aca5c674839ad74965193d4add2\n",
      "  hostname: kbPC\n",
      "  node_ip: 192.168.1.9\n",
      "  pid: 5672\n",
      "  timestamp: 1649236549\n",
      "  trial_id: '25056_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m 2022-04-06 11:15:49,465\tERROR function_runner.py:268 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     lambda cfg: self.train_model(cfg),\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     model.fit(\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     raise e.ag_error_metadata.to_exception(e)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m AssertionError: in user code:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         return step_function(self, iterator)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         outputs = model.train_step(data)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         y_pred = self(x, training=True)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     Call arguments received:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m       • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m       • training=True\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m       • mask=None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Exception in thread Thread-6:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 281, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     lambda cfg: self.train_model(cfg),\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     model.fit(\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     raise e.ag_error_metadata.to_exception(e)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m AssertionError: in user code:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         return step_function(self, iterator)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         outputs = model.train_step(data)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         y_pred = self(x, training=True)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m         assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m     Call arguments received:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m       • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m       • training=True\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m       • mask=None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5672)\u001b[0m \n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:49.779025: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:49.779047: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:49.782759: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:49.782880: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:51.021602: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:51.021651: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kbPC\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:51.021656: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kbPC\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:51.021756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.54.0\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:51.021781: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.54.0\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:51.021786: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.54.0\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m 2022-04-06 11:15:51.022004: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(bundle_reservation_check_func pid=5667)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:51.081352: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:51.081410: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kbPC\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:51.081418: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kbPC\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:51.081570: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.54.0\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:51.081604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.54.0\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:51.081609: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.54.0\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m 2022-04-06 11:15:51.082025: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=5663)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m 2022-04-06 11:15:51,342\tERROR function_runner.py:268 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     lambda cfg: self.train_model(cfg),\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     model.fit(\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     raise e.ag_error_metadata.to_exception(e)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m AssertionError: in user code:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         return step_function(self, iterator)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         outputs = model.train_step(data)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         y_pred = self(x, training=True)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     Call arguments received:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m       • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m       • training=True\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m       • mask=None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Exception in thread Thread-6:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 281, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     lambda cfg: self.train_model(cfg),\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     model.fit(\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     raise e.ag_error_metadata.to_exception(e)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m AssertionError: in user code:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         return step_function(self, iterator)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         outputs = model.train_step(data)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         y_pred = self(x, training=True)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m         assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m     Call arguments received:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m       • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m       • training=True\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m       • mask=None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m 2022-04-06 11:15:51,397\tERROR function_runner.py:268 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     lambda cfg: self.train_model(cfg),\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     model.fit(\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     raise e.ag_error_metadata.to_exception(e)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m AssertionError: in user code:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         return step_function(self, iterator)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         outputs = model.train_step(data)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         y_pred = self(x, training=True)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     Call arguments received:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m       • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m       • training=True\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m       • mask=None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Exception in thread Thread-6:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 281, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/util/tracing/tracing_helper.py\", line 451, in _resume_span\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     lambda cfg: self.train_model(cfg),\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     model.fit(\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     raise e.ag_error_metadata.to_exception(e)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m AssertionError: in user code:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         return step_function(self, iterator)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         outputs = model.train_step(data)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         y_pred = self(x, training=True)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m         assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m     Call arguments received:\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m       • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m       • training=True\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m       • mask=None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "2022-04-06 11:15:51,423\tERROR trial_runner.py:920 -- Trial lambda_25056_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/worker.py\", line 1763, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5667, ip=192.168.1.9, repr=<lambda>)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/trainable.py\", line 319, in train\n",
      "    result = self.step()\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 381, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 531, in _report_thread_runner_error\n",
      "    raise TuneError(\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5667, ip=192.168.1.9, repr=<lambda>)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "    lambda cfg: self.train_model(cfg),\n",
      "  File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "    model.fit(\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "AssertionError: in user code:\n",
      "\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\n",
      "    AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "    \n",
      "    Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "    \n",
      "    Call arguments received:\n",
      "      • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "      • training=True\n",
      "      • mask=None\n",
      "2022-04-06 11:15:51,466\tERROR trial_runner.py:920 -- Trial lambda_25056_00002: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 886, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 675, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/worker.py\", line 1763, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5663, ip=192.168.1.9, repr=<lambda>)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/trainable.py\", line 319, in train\n",
      "    result = self.step()\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 381, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 531, in _report_thread_runner_error\n",
      "    raise TuneError(\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=5663, ip=192.168.1.9, repr=<lambda>)\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 262, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 330, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 600, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 53, in <lambda>\n",
      "    lambda cfg: self.train_model(cfg),\n",
      "  File \"/home/kdbogusz/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py\", line 37, in train_model\n",
      "    model.fit(\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/kdbogusz/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1147, in autograph_handler\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "AssertionError: in user code:\n",
      "\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"/home/kdbogusz/.local/lib/python3.8/site-packages/keras/engine/functional.py\", line 598, in _run_internal_graph\n",
      "        assert x_id in tensor_dict, 'Could not compute output ' + str(x)\n",
      "\n",
      "    AssertionError: Exception encountered when calling layer \"model\" (type Functional).\n",
      "    \n",
      "    Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name=None), name='softmax/Softmax:0', description=\"created by layer 'softmax'\")\n",
      "    \n",
      "    Call arguments received:\n",
      "      • inputs=tf.Tensor(shape=(None, 8, 8), dtype=float32)\n",
      "      • training=True\n",
      "      • mask=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Model: \"model\"\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m _________________________________________________________________\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  Layer (type)                Output Shape              Param #   \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m =================================================================\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  input_1 (InputLayer)        [(None, 8, 8)]            0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  flatten (Flatten)           (None, 64)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  dense_1 (Dense)             (None, 64)                4160      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  dense_5 (Dense)             (None, 64)                4160      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  dense_10 (Dense)            (None, 16)                1040      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  dense_15 (Dense)            (None, 8)                 136       \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  gaussian_dropout (GaussianD  (None, 8)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  ropout)                                                         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  dense_16 (Dense)            (None, 10)                90        \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  softmax (Softmax)           (None, 10)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m =================================================================\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Total params: 9,586\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Trainable params: 9,586\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m _________________________________________________________________\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m ===\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m ===\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m [[[ 0.  0.  1. ...  9.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 13. ... 11.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 13. ...  9.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 11. ... 16.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  9. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  3. ...  2.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  8. ...  3.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  6. 15. ... 12.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 15.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 11. ... 15.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  8. ... 12.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  1. 10. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  7. 14. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  7. 11. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  4. 12.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 11. ... 10. 12.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 10. ... 14.  4.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  7. 12. ...  8.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  8. 16. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  2. 11. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  7. 16. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  4. 16. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  9. 12. ...  0.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  7. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  6. ... 16. 12.  5.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  5. ... 16. 16. 15.]]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m [8 1 3 ... 5 7 1]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m [[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  5.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  8. 12. ... 15.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 10.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  3.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  6. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  5. ... 10.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 15.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  9.  4.  1.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  3. ... 16. 16. 10.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  5. ...  9.  6.  2.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  0. ... 14.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 16.  2.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Model: \"model\"\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m _________________________________________________________________\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  Layer (type)                Output Shape              Param #   \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m =================================================================\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  input_1 (InputLayer)        [(None, 8, 8)]            0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  flatten (Flatten)           (None, 64)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  dense_1 (Dense)             (None, 64)                4160      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  dense_5 (Dense)             (None, 64)                4160      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  dense_10 (Dense)            (None, 16)                1040      \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  dense_15 (Dense)            (None, 8)                 136       \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  gaussian_dropout (GaussianD  (None, 8)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  ropout)                                                         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  dense_16 (Dense)            (None, 10)                90        \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  softmax (Softmax)           (None, 10)                0         \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m                                                                  \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m =================================================================\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Total params: 9,586\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Trainable params: 9,586\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m _________________________________________________________________\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m None\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m ===\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m ===\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m [[[ 0.  0.  1. ...  9.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 13. ... 11.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 13. ...  9.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 11. ... 16.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  9. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  3. ...  2.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  8. ...  3.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  6. 15. ... 12.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 15.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 11. ... 15.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  8. ... 12.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  1. 10. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  7. 14. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  7. 11. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  4. 12.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 11. ... 10. 12.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 10. ... 14.  4.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  7. 12. ...  8.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  8. 16. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  2. 11. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  7. 16. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  4. 16. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  9. 12. ...  0.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  7. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  2. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  6. ... 16. 12.  5.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  5. ... 16. 16. 15.]]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m [8 1 3 ... 5 7 1]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m [[[ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  5.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  8. 12. ... 15.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 10.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  3.  0.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  6. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  5. ... 10.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 15.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  9.  4.  1.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  3. ... 16. 16. 10.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  5. ...  9.  6.  2.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  0. ... 14.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 16.  2.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 14.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  6. 16. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 16.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 14.  1.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  0. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ...  9.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 15. ...  4.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  3. 12. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  0. ... 13.  7.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  0. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  8. ...  4.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  3. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  2. 16. ... 13.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 11. ... 16.  9.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  1. ... 10.  2.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  [[ 0.  0.  6. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0. 15. ... 15.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  4. 16. ...  9.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  4.  8. ...  9.  4.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  1. 13. ... 14.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m   [ 0.  0.  5. ...  4.  0.  0.]]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m [4 2 1 1 0 6 7 9 5 6 7 0 3 6 7 5 4 3 6 3 7 7 4 5 0 9 3 1 2 8 0 4 0 0 3 7 1\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  1 3 5 1 2 9 4 1 0 8 5 3 1 7 6 5 3 0 2 1 2 3 3 8 2 8 3 4 7 4 9 0 6 2 1 0 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  9 8 2 0 6 4 7 5 5 5 3 5 3 3 9 6 3 5 9 1 1 8 5 9 4 2 6 9 8 4 1 2 6 2 0 7 1\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  3 9 1 4 6 0 4 5 2 7 5 2 5 9 4 9 9 2 2 0 1 8 7 9 9 9 3 3 1 0 8 6 0 2 4 1 9\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  2 0 2 1 5 7 2 1 0 9 7 9 4 0 2 3 2 8 1 9 9 3 7 6 5 1 5 2 7 4 6 8 8 4 5 0 5\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  3 3 4 0 1 8 2 3 4 0 4 4 6 6 6 4 3 1 9 3 9 0 1 1 4 2 6 4 2 7 7 9 5 8 6 6 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  6 8 2 9 1 5 0 5 1 8 7 8 6 6 5 1 0 5 9 0 6 4 3 4 9 3 9 5 2 7 2 6 7 1 1 1 8\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  6 8 8 1 6 1 7 6 8 4 5 2 8 2 3 7 6 2 8 6 8 6 7 5 3 7 1 0 1 7 4 0 9 2 5 8 5\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  8 2 7 7 8 6 2 1 6 7 1 2 5 9 2 3 9 7 2 8 7 2 4 5 1 6 0 0 3 6 8 7 4 9 6 9 4\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  6 2 2 5 2 6 2 4 5 3 9 9 4 0 0 4 8 2 7 8 9 7 1 3 2 2 3 2 7 7 4 1 7 2 4 9 7\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  5 3 9 9 4 9 4 9 2 9 8 9 5 9 7 7 5 0 8 5 7 2 4 7 8 7 6 6 8 9 0 8 5 5 8 8 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  3 6 0 7 4 1 5 4 9 5 9 6 1 2 7 5 6 3 1 0 4 3 0 0 8 0 7 0 9 7 3 2 9 6 9 0 9\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m  4 6 7 1 6 0]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 14.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  6. 16. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 16.  1.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 14.  1.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  0. ... 11.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 13.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ...  9.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 15. ...  4.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  3. 12. ...  6.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  0. ... 13.  7.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  0. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  8. ...  4.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  3. ...  0.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  2. 16. ... 13.  7.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 11. ... 16.  9.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  1. ... 10.  2.  0.]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m \n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  [[ 0.  0.  6. ...  1.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0. 15. ... 15.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  4. 16. ...  9.  3.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   ...\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  4.  8. ...  9.  4.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  1. 13. ... 14.  0.  0.]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m   [ 0.  0.  5. ...  4.  0.  0.]]]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m [4 2 1 1 0 6 7 9 5 6 7 0 3 6 7 5 4 3 6 3 7 7 4 5 0 9 3 1 2 8 0 4 0 0 3 7 1\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  1 3 5 1 2 9 4 1 0 8 5 3 1 7 6 5 3 0 2 1 2 3 3 8 2 8 3 4 7 4 9 0 6 2 1 0 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  9 8 2 0 6 4 7 5 5 5 3 5 3 3 9 6 3 5 9 1 1 8 5 9 4 2 6 9 8 4 1 2 6 2 0 7 1\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  3 9 1 4 6 0 4 5 2 7 5 2 5 9 4 9 9 2 2 0 1 8 7 9 9 9 3 3 1 0 8 6 0 2 4 1 9\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  2 0 2 1 5 7 2 1 0 9 7 9 4 0 2 3 2 8 1 9 9 3 7 6 5 1 5 2 7 4 6 8 8 4 5 0 5\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  3 3 4 0 1 8 2 3 4 0 4 4 6 6 6 4 3 1 9 3 9 0 1 1 4 2 6 4 2 7 7 9 5 8 6 6 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  6 8 2 9 1 5 0 5 1 8 7 8 6 6 5 1 0 5 9 0 6 4 3 4 9 3 9 5 2 7 2 6 7 1 1 1 8\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  6 8 8 1 6 1 7 6 8 4 5 2 8 2 3 7 6 2 8 6 8 6 7 5 3 7 1 0 1 7 4 0 9 2 5 8 5\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  8 2 7 7 8 6 2 1 6 7 1 2 5 9 2 3 9 7 2 8 7 2 4 5 1 6 0 0 3 6 8 7 4 9 6 9 4\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  6 2 2 5 2 6 2 4 5 3 9 9 4 0 0 4 8 2 7 8 9 7 1 3 2 2 3 2 7 7 4 1 7 2 4 9 7\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  5 3 9 9 4 9 4 9 2 9 8 9 5 9 7 7 5 0 8 5 7 2 4 7 8 7 6 6 8 9 0 8 5 5 8 8 0\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  3 6 0 7 4 1 5 4 9 5 9 6 1 2 7 5 6 3 1 0 4 3 0 0 8 0 7 0 9 7 3 2 9 6 9 0 9\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m  4 6 7 1 6 0]\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5667)\u001b[0m Epoch 1/12\n",
      "\u001b[2m\u001b[36m(<lambda> pid=5663)\u001b[0m Epoch 1/12\n",
      "Result for lambda_25056_00001:\n",
      "  date: 2022-04-06_11-15-51\n",
      "  experiment_id: e43ee00abcdc48c4bb1a60e48bc6e64e\n",
      "  hostname: kbPC\n",
      "  node_ip: 192.168.1.9\n",
      "  pid: 5667\n",
      "  timestamp: 1649236551\n",
      "  trial_id: '25056_00001'\n",
      "  \n",
      "Result for lambda_25056_00002:\n",
      "  date: 2022-04-06_11-15-51\n",
      "  experiment_id: 534590f1134247e5b3942f904489ac42\n",
      "  hostname: kbPC\n",
      "  node_ip: 192.168.1.9\n",
      "  pid: 5663\n",
      "  timestamp: 1649236551\n",
      "  trial_id: '25056_00002'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-06 11:15:51 (running for 00:00:04.16)<br>Memory usage on this node: 5.2/13.5 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 320.000: None | Iter 80.000: None | Iter 20.000: None<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/5.96 GiB heap, 0.0/2.98 GiB objects<br>Result logdir: /home/kdbogusz/ray_results/exp<br>Number of trials: 3/3 (3 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">       lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_25056_00000</td><td>ERROR   </td><td>192.168.1.9:5672</td><td style=\"text-align: right;\">0.0467722</td></tr>\n",
       "<tr><td>lambda_25056_00001</td><td>ERROR   </td><td>192.168.1.9:5667</td><td style=\"text-align: right;\">0.0341257</td></tr>\n",
       "<tr><td>lambda_25056_00002</td><td>ERROR   </td><td>192.168.1.9:5663</td><td style=\"text-align: right;\">0.0133466</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lambda_25056_00000</td><td style=\"text-align: right;\">           1</td><td>/home/kdbogusz/ray_results/exp/lambda_25056_00000_0_lr=0.046772_2022-04-06_11-15-47/error.txt</td></tr>\n",
       "<tr><td>lambda_25056_00001</td><td style=\"text-align: right;\">           1</td><td>/home/kdbogusz/ray_results/exp/lambda_25056_00001_1_lr=0.034126_2022-04-06_11-15-49/error.txt</td></tr>\n",
       "<tr><td>lambda_25056_00002</td><td style=\"text-align: right;\">           1</td><td>/home/kdbogusz/ray_results/exp/lambda_25056_00002_2_lr=0.013347_2022-04-06_11-15-49/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [lambda_25056_00000, lambda_25056_00001, lambda_25056_00002])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m teacher \u001b[38;5;241m=\u001b[39m Teacher(X, y)\n\u001b[1;32m     14\u001b[0m ga_instance \u001b[38;5;241m=\u001b[39m pg\u001b[38;5;241m.\u001b[39mGA(\n\u001b[1;32m     15\u001b[0m     num_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     16\u001b[0m     initial_population\u001b[38;5;241m=\u001b[39mteacher\u001b[38;5;241m.\u001b[39mcreate_initial_population(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     on_generation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m ga_instance: OnGenerationCallback()\u001b[38;5;241m.\u001b[39mon_generation(ga_instance)\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[43mga_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m solution, solution_fitness, solution_idx \u001b[38;5;241m=\u001b[39m ga_instance\u001b[38;5;241m.\u001b[39mbest_solution()\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m teacher\u001b[38;5;241m.\u001b[39mmap_chromosome_to_network_architecture(solution)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pygad/pygad.py:1192\u001b[0m, in \u001b[0;36mGA.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m stop_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Measuring the fitness of each chromosome in the population. Save the fitness in the last_generation_fitness attribute.\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_generation_fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcal_pop_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m best_solution, best_solution_fitness, best_match_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_solution(pop_fitness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_generation_fitness)\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;66;03m# Appending the best solution in the initial population to the best_solutions list.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pygad/pygad.py:1159\u001b[0m, in \u001b[0;36mGA.cal_pop_fitness\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_generation_fitness[parent_idx]\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitness_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43msol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msol_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(fitness) \u001b[38;5;129;01min\u001b[39;00m GA\u001b[38;5;241m.\u001b[39msupported_int_float_types:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(solution, solution_idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m X, y \u001b[38;5;241m=\u001b[39m load_digits()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m], load_digits()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m teacher \u001b[38;5;241m=\u001b[39m Teacher(X, y)\n\u001b[1;32m     14\u001b[0m ga_instance \u001b[38;5;241m=\u001b[39m pg\u001b[38;5;241m.\u001b[39mGA(\n\u001b[1;32m     15\u001b[0m     num_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     16\u001b[0m     initial_population\u001b[38;5;241m=\u001b[39mteacher\u001b[38;5;241m.\u001b[39mcreate_initial_population(),\n\u001b[1;32m     17\u001b[0m     gene_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     18\u001b[0m     num_parents_mating\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m---> 19\u001b[0m     fitness_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m solution, solution_idx: \u001b[43mteacher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolution_idx\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     20\u001b[0m     mutation_percent_genes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[1;32m     21\u001b[0m     on_generation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m ga_instance: OnGenerationCallback()\u001b[38;5;241m.\u001b[39mon_generation(ga_instance)\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m ga_instance\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m     24\u001b[0m solution, solution_fitness, solution_idx \u001b[38;5;241m=\u001b[39m ga_instance\u001b[38;5;241m.\u001b[39mbest_solution()\n",
      "File \u001b[0;32m~/repos/network-architecture-optimization/network_architecture_optimization/genetic_algorithms/teacher.py:26\u001b[0m, in \u001b[0;36mTeacher.fitness_function\u001b[0;34m(self, solution, solution_idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitness_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, solution: np\u001b[38;5;241m.\u001b[39mndarray, solution_idx: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_chromosome_to_network_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m~/repos/network-architecture-optimization/network_architecture_optimization/genetic_algorithms/teacher.py:23\u001b[0m, in \u001b[0;36mTeacher.map_chromosome_to_network_architecture\u001b[0;34m(self, chromosome)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_chromosome_to_network_architecture\u001b[39m(\u001b[38;5;28mself\u001b[39m, chromosome: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     22\u001b[0m     data \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_train\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_test\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test }\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchitecture_mapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_chromosome\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchromosome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/network-architecture-optimization/network_architecture_optimization/mappers/architecture_mapper.py:16\u001b[0m, in \u001b[0;36mAchitectureMapper.map_chromosome\u001b[0;34m(self, input_shape, chromosome, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_chromosome\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape, chromosome: np\u001b[38;5;241m.\u001b[39mndarray, data):\n\u001b[1;32m     15\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_layers(input_shape, chromosome)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/network-architecture-optimization/network_architecture_optimization/mappers/architecture_mapper.py:30\u001b[0m, in \u001b[0;36mAchitectureMapper._build_model\u001b[0;34m(self, inputs, outputs, data)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, outputs, data):\n\u001b[0;32m---> 30\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mHyperparameterOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py:76\u001b[0m, in \u001b[0;36mHyperparameterOptimizer.get_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 76\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_model(config)\n",
      "File \u001b[0;32m~/repos/network-architecture-optimization/network_architecture_optimization/ray_tuning/hyperparameter_optimization.py:52\u001b[0m, in \u001b[0;36mHyperparameterOptimizer.tune_mnist\u001b[0;34m(self, num_training_iterations)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtune_mnist\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_training_iterations):\n\u001b[1;32m     50\u001b[0m     sched \u001b[38;5;241m=\u001b[39m AsyncHyperBandScheduler(time_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, grace_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_iteration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_training_iterations\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found were: \u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis\u001b[38;5;241m.\u001b[39mbest_config)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m analysis\u001b[38;5;241m.\u001b[39mbest_config\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/tune/tune.py:633\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incomplete_trials:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failed_trial \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[signal\u001b[38;5;241m.\u001b[39mSIGINT]:\n\u001b[0;32m--> 633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrials did not complete: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [lambda_25056_00000, lambda_25056_00001, lambda_25056_00002])"
     ]
    }
   ],
   "source": [
    "from network_architecture_optimization.genetic_algorithms.callbacks import OnGenerationCallback\n",
    "from network_architecture_optimization.genetic_algorithms.teacher import Teacher\n",
    "import pygad as pg\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "X, y = load_digits()['images'], load_digits()['target']\n",
    "teacher = Teacher(X, y)\n",
    "ga_instance = pg.GA(\n",
    "    num_generations=10,\n",
    "    initial_population=teacher.create_initial_population(),\n",
    "    gene_type=int,\n",
    "    num_parents_mating=4,\n",
    "    fitness_func=lambda solution, solution_idx: teacher.fitness_function(solution, solution_idx),\n",
    "    mutation_percent_genes=60,\n",
    "    on_generation=lambda ga_instance: OnGenerationCallback().on_generation(ga_instance)\n",
    ")\n",
    "ga_instance.run()\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "model = teacher.map_chromosome_to_network_architecture(solution)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8b4d4da430775f898a8f023f50a1a0d3118352757845b449d2a0351d5961cd4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
